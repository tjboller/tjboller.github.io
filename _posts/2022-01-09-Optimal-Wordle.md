---
layout: post
title:  "How To Play Perfect Wordle"
date:   2022-01-06
categories: jekyll update
---

# Intro

If your twitter feed looks anything like mine, you are probably seeing 
a lot of posts that look something like this:

```
Wordle 201 5/6

â¬› ðŸŸ© â¬› â¬› â¬›
â¬› ðŸŸ© ðŸŸ© â¬› â¬›
ðŸŸ¨ ðŸŸ© ðŸŸ¨ â¬› â¬›
ðŸŸ© ðŸŸ© ðŸŸ¨ â¬› â¬›
ðŸŸ© ðŸŸ© ðŸŸ© ðŸŸ© ðŸŸ©
```

It's from a great daily game called <a href="https://www.powerlanguage.co.uk/wordle/" target="_blank">Wordle</a>. If you haven't played it, click the link and
play. It's very short and will help make this post make a lot more sense.

# Solver

[This lil app](https://share.streamlit.io/tjboller/wordle/main/visualizations.py)
helps you solve the puzzles optimally and explores how good different start
 words are - check it out.

# Summary

My analysis shows that Wordle is mostly a game of luck.
The best word to start with is `L A R E S` and I have programmed an [optimal
solver](https://share.streamlit.io/tjboller/wordle/main/visualizations.py) but 
as long as you don't do stupid stuff, you probably can't tell the difference
between someone who is playing optimally and someone who just doesn't make 
mistakes or really dumb moves. Just play the game and have fun lol

# What is the best start word?

The answer is `L A R E S`

How did we get that? First we have to define what do we mean by the best.
It's easiest to start with an example:

Assume the hidden answer word is `T I G E R` and your start word is `E A R T H`.

The game will tell you that the hidden answer word:
- Has a `E` somewhere in the word, but in not as the first letter
- Does not have an `A`
- Does not have an `R`
- Has a `T` somewhere in the word, but not in the fourth spot
- Does not have an `H`

There are 12,972 total five letter words as described in the <a href=" https://drive.google.com/file/d/1oGDf1wjWp5RF_X9C7HoedhIWMh5uJs8s/view" target="_blank">scrabble 
dictionary</a>. But there are only 144 words that fit all of these criteria. 

So obviously, we don't know what the hidden answer is, but what we can do is
say how many words are possible after our first guess 
if the answer is `T I G E R`, 
and follow the same process if is the answer is `W O R L D`,
and `A B O U T` and every single other possible five letter word. So if we
take the average of every single possible answer, 
we can say how many words we *expect* to have left after we guess our 
start word.  (It's the plain average since every word is equally 
likely to be the answer*)

The best guess will result in the fewest words left to choose from on average
across all possible answers.

Here is the distribution of how many words are left if you first guess 
`EARTH` across every single possible answer. You can look at the distributions 
any guess word [here](https://share.streamlit.io/tjboller/wordle/main/visualizations.py).

<img src="{{site.url}}/resources/guess_histogram.png" alt="hist">

Average number of words remaining after guessing `EARTH`: 670.5

If we measure this for all possible start words, we can say with absolute 
certainty (see complications below) which start word is best on expectation. 

Here are the top 15 best words.

<table border="1" class="dataframe"><thead><tr style="text-align: right;"><th>Guess</th><th>Average Possible Words After Guess</th></tr></thead><tbody><tr><td>lares</td><td>288.738205</td></tr><tr><td>rales</td><td>292.106075</td></tr><tr><td>tares</td><td>302.491674</td></tr><tr><td>soare</td><td>303.830866</td></tr><tr><td>reais</td><td>304.761024</td></tr><tr><td>nares</td><td>305.548720</td></tr><tr><td>aeros</td><td>309.733426</td></tr><tr><td>rates</td><td>311.355381</td></tr><tr><td>arles</td><td>314.733888</td></tr><tr><td>serai</td><td>315.133364</td></tr><tr><td>saner</td><td>317.736047</td></tr><tr><td>tales</td><td>325.594820</td></tr><tr><td>aloes</td><td>326.700123</td></tr><tr><td>reals</td><td>329.109004</td></tr><tr><td>lanes</td><td>330.211224</td></tr></tbody></table>

Here are the 5 worst words

<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th>word</th>      <th>count</th>    </tr>  </thead>  <tbody>    <tr>      <td>hudud</td>      <td>4454.262411</td>    </tr>    <tr>      <td>phpht</td>      <td>4458.732809</td>    </tr>    <tr>      <td>gynny</td>      <td>4677.415665</td>    </tr>    <tr>      <td>qajaq</td>      <td>4960.594665</td>    </tr>    <tr>      <td>fuzzy</td>      <td>5041.903639</td>    </tr>    <tr>      <td>cocco</td>      <td>5046.276133</td>    </tr>    <tr>      <td>hyphy</td>      <td>5237.960839</td>    </tr>    <tr>      <td>immix</td>      <td>5248.980419</td>    </tr>    <tr>      <td>gyppy</td>      <td>5384.723558</td>    </tr>    <tr>      <td>fuffy</td>      <td>5396.426303</td>    </tr>  </tbody></table>

## Playing Optimally

If we play optimally what is the distribution of the number of turns it takes
to win?

<img src="{{site.url}}/resources/play_optimally.png" alt="optimal">

Kinda surprisingly, we still lose a good amount of games. Let's see if we try 
a non-optimal but not dumb first word. 

<img src="{{site.url}}/resources/earth_vs_lares.png" alt="earth_vs_lares">

Wow ok, so these distributions are the probably almost exactly 
the same (ks-test pvalue=0.98).
 It's important to note, that after the first word we are playing optimally, so 
 `EARTH` as a start word quickly catches up. Look at the history of number 
 of possible words after the turn number:
 
<p float="left">
    <img src="{{site.url}}/resources/guess_history.png" alt="history">
</p>

Most people can't play optimally, so what if your strategy was to pick
 the most frequently used
possible remaining word - what would you score distribution look like then?

<img src="{{site.url}}/resources/optimal_vs_freq_lares.png" alt="opt_vs_freq">

Certainly better if you play optimally, but doesn't seem like that all that
big of a qualitative difference. What about if were an average player, who picks
a non-optimal but still pretty good start word and then after words just picks
the most common word remaining?

<img src="{{site.url}}/resources/optimal_vs_average.png" alt="opt_vs_avg">

Yea again, it's worse but probably not all that noticeable in real life.
So don't be like me, just play the game for fun and move on with your life. 

#### Complications

In this analysis we assume that all valid 5 letter words are equally likely to
be the answer. I think this is correct, given some recent odd answers like 
`REBUS` and `BANAL` but it could not be. All of this still works if the answers
are randomly selected by the probability of frequency of use, the math just 
gets a little trickier see the section "what if the answers aren't selected randomly"

Another complication is that for a single guess, 
we need to evaluate how good that guess is across all 
possible words. And we need to evaluate all possible words. That's a LOT 
of computation. `12,972^2 = 168,272,784` to be exact. If I ran that on a 
single thread for every possibility it would take many many days to run. 
I speed it up by 
1. Parallelizing across my cores
1. Randomly sampling 100 possible answers across first to get the top 
250 possible best first guesses, then rerunning those on all possible answers.

Also, this is using a greedy approach. It might not be globally optimal. The 
global optimization problem is completely computationally intractable (as far
as I can tell).

## What if the answers aren't selected randomly

If every answer is not equally likely, it would make sense then that every
word has a probability of being the answer with probability proportional 
to its frequency of use. 

This mainly changes two things: 
1. Instead of tracking the number of possible words. We need to track the 
entropy of the probability distribution of the remaining words. Entropy 
measures how much uncertainty there is in a probability distribution. For 
example, a uniform distribution between 0 and 10 has less entropy than a 
uniform distribution between 0 and 100. Similarly, a uniform distribution 
has more distribution than a "pointy" distribution over the same interval
(the shape of the distribution helps us guess what the outcome would be)
2. Instead of just taking the average entropy across all words, we need to 
take the expectation. Namely, the probability of the word being the answer
 times the entropy of the remaining words summed across all possible words.

# Repo

Check out the code powering this anaylsis [here](https://github.com/tjboller/wordle)